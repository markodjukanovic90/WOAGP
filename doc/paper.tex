\documentclass[runningheads,a4paper]{elsarticle}
% vim: tw=0 wm=0

\setcounter{tocdepth}{3}
\usepackage{amssymb}
\usepackage{amsmath}
%\usepackage{amsthm}
\usepackage{bbm}
\usepackage{environ}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{comment}
\usepackage{placeins}
\usepackage{mathtools}
%\usepackage{algorithmic}
\usepackage{enumitem}
\usepackage[utf8]{inputenc}
%\usepackage{enumite}
%\usepackage{cleveref}
%\usepackage{parskip}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{array}
\usepackage[pdfencoding=auto,psdextra]{hyperref}
\usepackage{booktabs}
\usepackage{bookmark}% faster updated bookmarks
\usepackage{hypcap} % fix the links
\evensidemargin\oddsidemargin
\usepackage{graphicx}
\pagestyle{plain}
\usepackage{xcolor}
\newcommand\ToDo[1]{\textcolor{red}{#1}}
%\bibliographystyle{plainnat}
\usepackage{siunitx}
\usepackage{color}

\usepackage[draft,nomargin,inline]{fixme}
\fxsetface{inline}{\itshape}
\fxsetface{env}{\itshape}
%\fxuselayouts{margin}
%\fxuselayouts{inline}
\fxusetheme{color}

\usepackage{url}
\urldef{\mailsa}\path|{djukanovic, raidl}@ac.tuwien.ac.at,|
\urldef{\mailsb}\path|christian.blum@iiia.csic.es|
\newcommand{\keywords}[1]{\par\aDSvspace\baselineskip
	\noindent\keywordname\enspace\ignorespaces#1}

\usepackage{tikz}
\usetikzlibrary{positioning}
\definecolor{canaryyellow}{rgb}{1.0, 0.94, 0.0}
\definecolor{brightgreen}{rgb}{0.4, 1.0, 0.0}
\definecolor{jazzberryjam}{rgb}{0.65, 0.04, 0.37}

%defining of command

\newcommand\floor[1]{\lfloor#1\rfloor}
\newcommand\ceil[1]{\lceil#1\rceil}
\newcommand\str[1]{\texttt{#1}}
\newcommand\pL[1][]{\ensuremath{p^{\mathrm{L}#1}}}
\newcommand\pR[1][]{\ensuremath{p^{\mathrm{R}#1}}}
\newcommand\qL{\ensuremath{q^\mathrm{L}}}
\newcommand\qR{\ensuremath{q^\mathrm{R}}}
\newcommand\pLH{\ensuremath{\hat{p}^\mathrm{L}}}
\newcommand\pRH{\ensuremath{\hat{p}^\mathrm{R}}}
\newcommand{\Vext}{\ensuremath{V_\mathrm{{ext}}}}
\newcommand\UB{\ensuremath{\mathrm{UB}}}
\newcommand\Sigmand{\ensuremath{\Sigma^\mathrm{nd}}}
\renewcommand{\labelenumii}{\theenumii}
\renewcommand{\theenumii}{\theenumi.\arabic{enumii}.}
\setlength{\leftmarginii}{1.8ex}
\raggedbottom
\algnewcommand\algorithmicforeach{\textbf{for each}}
\algdef{S}[FOR]{ForEach}[1]{\algorithmicforeach\ #1\ \algorithmicdo}

% scaling factor for tables
\newcommand\tabscale{0.8}

\begin{document}
	
	%\setlength{\parindent}{0pt}  % disallow indentations
	%\numberwithin{table}{1}
	%\mainmatter  % start of an individual contribution
	
	% first the title is needed
	\title{Measuring efficiency of the greedy-like heuristics for solving  the Weighted Orthogonal Art Gallery Problem under regular grid discretization}
	
	%
	%  \author{--}
	%
	
	%  \institute{%$^1$Institute of Logic and Computation, TU Wien,
	%Vienna, Austria,\\
	%	       $^2$ Artificial Intelligence Research Institute (IIIA-CSIC),\\ \normalsize Campus UAB, Bellaterra, Spain \\
	%\institute{Springer-Verlag, Computer Science Editorial,\\
	%Tiergartenstr. 17, 69121 Heidelberg, Germany\\
	%\mailsa\\
	%\mailsb\\
	%\mailsc\\
	%\url{http://www.springer.com/lncs}
	%}
	\begin{abstract}
		In this paper...
	\end{abstract}
	\maketitle
	
	
	\section{Introduction}\label{sec:introduction}
	
	For a given polygon $P$, the \emph{Art Gallery Problem} (AGP) asks for a set of points $G$ of minimal cardinality,  such that for each point $y \in P$ there is $x \in G$ such that $xy \subset P$. We say that the point $y$ is covered by the point $x$, or $y$ is visible from $x$. Set $G$ is called \emph{guard set} of $P$ and the points from $G$ as \emph{guards}. In the \emph{Orthogonal Art Gallery Problem} (OAGP) we suppose that edges of the polygon are only horizontal and vertical w.r.t. the axes, i.e.  the angles allowed between adjacent edges are  $90^{\circ}$ or $270^{\circ}$. The original AGP was initially stated by Victor  Klee in 1973.~\cite{o1987art}.  The problem is motivated from installing the cameras inside a building (or gallery) such that the whole area of the building is covered. Orthogonality constraint naturally comes out from the orthogonality of the walls in buildings. Kahn et al.~\cite{kahn1983traditional} formulated and proofed that 	$\lfloor \frac{n}{4} \rfloor$ guards are  sufficient to cover an orthogonal polygon with $n$ vertices.      In this study, we are interested in the variant of the OAGP which allows only that guards are positioned at the vertices of polygon $P$. This restricted problem is known to be $\mathcal{NP}$--hard~\cite{schuchardt1995two,katz2008guarding}.  When it comes to the real situations (like installing the cameras in a building), it is justified to assume that the prices of cameras are not equal and may depends on several factors, like the quality of a camera   (respecting its range of spectrum of view)  or installation price at some specific parts of the building (like corners or tight places).  In the \emph{Weighted Orthogonal Art Gallery Problem} (WOAGP), which we finally consider in this paper, the task is to place guards on some vertices of the orthogonal polygon which cover all points from $P$, such that the total sum of prices assigned to the chosen vertices is minimal. \fxnote{Marko: Ovo nije nas problem, nas je relaksiraniji, posmatramo ga nad diskretizacijom. Mislim da to treba odmah naglasiti ovdje i razlog asto uzimamo ovu diskretizaciju...}
	
	It is well known that AGP can be reduced to \emph{the Minimum Set Cover Problem} (MSCP) by a discretization of the set of all points of the polygon $P$. The appropriate discretization should be performed in such a way that if each point from the discretized set $D(P)$ is covered, then the whole polygon $P$ is covered. After the discretization is made, for each vertex of the polygon $P$, a set of visible points from $D(P)$ is determined. In that way, the problem of determining the minimum number of guards covering the entire polygon is reduced to determining the minimum number of subsets of points, such that each point from $D(P)$ is included in at least one of the chosen subsets, which is MSCP. Analogously, WOAGP can be reduced to the \emph{Weighted Minimum Set Cover Problem} (WMSCP).
	
	Concerning the exact and heuristic techniques to solve OAGP, Couto et al.~\cite{couto2007exact} presented an exact and efficient exact algorithm for the OAGP based on preprocessing and refinement phases of the the discretized instance. In ~\cite{ghosh2010approximation} an approximate solution of the minimum vertex guard problem, which can be computed in $O(n^4)$ time and the size of the solution is at most $O(\log n)$ times the optimal. After that, on these constructed sets Johnsonâ€™s approximation algorithm ~\cite{johnson1974approximation} for the MSCP is applied. An anytime algorithm to compute successively better approximations of the optimum to Minimum Vertex Guard is proposed in ~\cite{tomas2003approximation}.  A major idea of this approach is exploring dominance of visibility regions to first detect pieces that are
	more difficult to guard. The same problem is solved   in ~\cite{tomas2006visibility} by applying successive approximations from  ~\cite{tomas2003approximation}.
	Tozoni et a. ~\cite{tozoni2013practical,tozoni2016algorithm}  presented an exact \emph{Integer Linear Programming}  (ILP)-based  algorithm, which iteratively generates upper and lower bounds through the resolution of discretized space of the AGP. Although many variants AGP are present in literature, WOAGP has not been so intensively studied, which motivated us to consider this problem.
	A comprehensive analysis of various greedy-like heuristics for the WMSCP was presented in \cite{vasko2016best}.
	More detailed overview of the extensive literature regarding SCP and AGP is out of the scope of this paper and for further reading we suggest review papers ~\cite{caprara2000algorithms,ren2010new,ghosh2010approximation2}. 

	
	\subsection{Main contributions}
	The main contributions of this paper are:
	\begin{itemize}
		\item We developed a novel greedy approach which is based on balancing the trade off between the total sum of guards' costs and the total number of not yet covered points from the discretization.
		\item We developed a specially designed shaking procedure, which perturbs the current partial solution in such a way that some vertices are replaced with others, where the replacement is based on measuring  distances between points in the polygon.\fxnote{TODO: popraviti}
		\item Both an existing and the novel greedy algorithms are hybridized with the proposed shaking procedure and with the ILP.
		\item We considered different types of weights for our benchmarks, based on an approximation of the costs in real situations.
		\item In a comprehensive computational experiment, \fxnote{TODO: popraviti} different variants of approximative algorithms we tested, analysed, and their efficiency checked. The methods are then compared to the exact approaches ILP and \emph{Constraint Programming} (CP).
	\end{itemize}

	
	\subsection{The Regular Discretization of Polygon}  
	The discretization $D(P)$ of the polygon $P$ is built as follows. Consider the regular grid with resolution $\bigtriangleup_{x}\times\bigtriangleup_{y}$ starting at the lower left corner of the bounding box of polygon $P$ which height and weight are defined as follow: 
 
	\begin{equation}
         \bigtriangleup_{x}=\min\{ |u_{x}-v_{x}|\mid u_{x}\neq v_{x}\} \mbox{ and }
         \bigtriangleup_{y}=\min\{ |u_{y}-v_{y}|\mid u_{y}\neq v_{y}\},
 	\end{equation}
where $ (u_{x},u_{y}),(v_{x},v_{y})$ are adjacent vertices of polygon $P$.
	
	\begin{algorithm}[!t]
		\caption{Discretization $D(P)$ of polygon $P$}\label{alg:discret}
		\begin{algorithmic}[1]
			\State \textbf{Input:} The set of vertices $V$ of polygon $P$
			\State \textbf{Output:} The discretization $D(P)$ of polygon $P$
			\State $BB \gets bounding\_box(P);$
			\State $\bigtriangleup_{x},\bigtriangleup_{y} \gets resolution(P);$
			\State $D(BB) \gets regular\_grid(BB,\bigtriangleup_{x},\bigtriangleup_{y});$
			\State $D(P) \gets D(BB) \cap P;$
			\State $D(P) \gets D(P) \cup V;$
		\end{algorithmic}
	\end{algorithm}

	All intersection points of this regular grid with polygon $P$ and all vertices of $P$ are added into the discrete set $D(P)$. The pseudocode which presents the building steps of the discretizations $D(P)$ is given in Algorithm~\ref{alg:discret}.
	
	\fxnote{TODO: zasto uzimamo ovakav tip diskretizacije, objasniti zasto id pokrivenosti diskretizacije ne slijedu pokrivenost poligona -- dvije slike dodaj gdje optimalan izbor cvorova pokriva poligon, i gdje optimalni izbor cvorova ne pokriva poligon...}
	
	\section{Exact methods}
	In this section we present the exact ILP and \emph{Constraint Programming} (CP) models for solving WMSCP  under regular grid discretization, which are used  in the rest of the paper.
	\subsection{Integer linear programming model}
	Let us suppose we are given a polygon $P$ and a discretization $D(P)$ of $P$.  The task we consider is covering all points from $D(P)$ by some vertices $V=\{v_1,...,v_n\}$ of $P$ such that the sum of their weights is minimized.
	The problem is related to the known MWSC problem.
	Family $\mathcal{F}$ of nonempty sets consists of the sets
	$S_i \in \mathcal{F}$ which include points $p \in D(P)$ that are visible from guard $v_i\in V$, i.e., $pv_i \subset P$.  Note that set $S_i$ includes points $p\in D(P)$ that might also be included by some other guard $v_j\in V$, $i \neq j$. For each set $S_i$, the cost $c(S_i) = w_i$ is assigned.  In this way, our starting task is equivalent of finding a minimum cardinalty covering $\mathcal{C}\subseteq\{S_1,...,S_n\}$ of the set of points $D(P)$, that is
	$$ \bigcup_{C \in \mathcal{C}} C = D(P),$$ such that $\sum_{S_i \in \mathcal{C}} c(S_i)$ is minimized.  The ILP  model for the WSCP is already known from the literature, where  here adapted to WOAGP  as follows:

	\begin{align}
	&\sum_{i=1}^n w_ix_i \longrightarrow \min \\
	&\mbox{s.t.} \\
	&\sum_{j\in V} a_{ij}x_j \geq 1\ (\forall p_i\in D(P)) \label{eq:const-3}\\
	& x_j \in \{0,1\}, j \in V,
	\end{align}
	where
	$a_{ij} = \begin{cases}
	1, p_i \in V(j), \\
	0, \mbox{otherwise}
	\end{cases}$
	and $x_i = \begin{cases}
	1, \mbox{ if } \mbox{ the point } p_i \in \mathcal{C},\\
	0, \mbox{otherwise},
	\end{cases}$ \\

	where $V(j)$ is the set of all points from $D(P)$ that are visible from $j$-th vertex of $P$.
	Set $Z = \{j \in V\mid x_i=1\}$ represents a solution of the problem w.r.t. discretization $D(P)$ of polygon $P$.
	Constraint~(\ref{eq:const-3}) enforces that any point $p_i \in D(P)$ will be visible from at least one guard from $Z$. 
	
	In order to solve this model, we apply a general purpose solver \textsc{Cplex}~\cite{lima2010ibm}.
	\subsection{Constraint programming model} An equivalent CP model was implemented and tested by IBM ILOG CP Optimizer~\cite{laborie2018ibm}. In this case,  Constraint~(\ref{eq:const-3}) is transformed into
	\begin{equation}
	\bigvee_{ j \in V } (a_{ij} \wedge x_j) = 1,
	\end{equation}
	whereas the other constraints and the objective function are the same like in the above MIP model. Note that CP appraoch works in a branch-and-bound manner employing a constraint propagation and variable domain filtering~\cite{rossi2006handbook}.
	\section{Algorithmic Approaches for WOAGP}
	\subsection{Greedy approaches for solving WOAGP}
	Since the WOAGP instances can be transformed into  WMSCP instances and since one of the most efficient heuristics to solve WSCP from the literature is an enhanced greedy heuristic~\cite{chvatal1979greedy}, our idea is after transforming a WOAGP instance into a WSC problem instance to	apply various greedy algorithms to solve WOAGP. Greedy algorithms produce a solution of reasonable quality within a short interval of time and are, in essence, easy to implement. Efficiency of such greedy heuristics is related to a greedy criterion utilized to expand current (non-complete, i.e., partial) solution to complete one. Among all candidates (solution components for expansion, that is not-yet-considered guards) to extend current partial solution,  we choose one with the smallest greedy value and add it to the current solution until it becomes complete (i.e., it covers all  points from $D(P)$).
	
	A general pseudocode of Greedy heuristics is given in Algorithm~\ref{alg:greedy}
	
	\begin{algorithm}[!t]
		\caption{Greedy Heuristic}\label{alg:greedy}
		\begin{algorithmic}[1]
			\State \textbf{Input:} an instance of a problem
			\State \textbf{Output:} A (feasible) non-expandable solution (or reporting that no feasible solution)
			\State $s^{P} \gets ()$ \hspace{0.3cm}// partial solution set to empty solution
			\While{$\text{Extend}(s^{P}) \neq \emptyset$}
			\State Select component $e \in  \text{Extend}(s^{P})$ \hspace{0.3cm}//\,w.r.t.\  some criterion $g$
			\State Extend $s^{P}$ by $e$
			\EndWhile
		\end{algorithmic}
	\end{algorithm}
	\subsubsection{An existing greedy method} 
	
	%  \subsection{Greedy Criterion based on Price-per-Unit}
	Concerning the greedy heuristic for the literature for solving WMSC problem~\cite{chvatal1979greedy, lovasz1975ratio}, one of the most efficient greedy heuristic was based on the following greedy rule:
	\begin{align}
	g(s^p, p_i) = \frac{w_{p_i}}{ f(s^p \cup \{p_i\})  - f(s^p)},
	\end{align}
	where $f(s^p) = |\bigcup_{s \in s^p} s |$.
	This heuristic is also an approximation algorithm with $O(\log(n))$ approximation factor.
	
	\subsubsection{A Novel Greedy Heuristic} 
	In this subsection we present a novel greedy function, which is used in the greedy algorithm. First, we introduce a term  ``incorrect point''; for a point from $D(P)$ we say that it is incorrect if it is not covered by any guard from a current partial solution $s^{ps}$. Let us denote by $incorrect_{total}$ the total number of incorrect points from discretization $D(P)$ for the current partial solution.  Let $w_{total}$ be the total sum of all weights among all vertices and $|D(P)|$ be the cardinality of discretization set. The greedy function for the given partial solution $s^{ps}$ and the guard $v$ takes into account both the value of the objective function of the WOAGP  applied to the partial solution when guard $v$ is included in $s^{ps}$ and the total number of incorrect points, given by the formula:
	\begin{equation}\label{eq:greedyfun2}
	%$$obj(s^{ps}; v)  =    \frac{\sum_{i \in s^{ps} \cup \{v\}\}} w_i}{w_{total}}+ incorrect_{total}$$
	g(s^{ps}; v) = \alpha \cdot \sum_{i \in s^{ps} \cup \{v\}} w_i+ \beta \cdot {incorrect_{total}}
	\end{equation}
	where $\alpha$ and $\beta$ are positive real parameters.
	
	Although many combination of parameters $ \alpha $ and $\beta$ may be used, in our investigation we used parameter $\alpha$  only to normalize the sum of prices, i.e. $\alpha = \frac{1}{w_{total}}$ and two values for $\beta$: $\beta = 1$ and $\beta = \frac{1}{|D(P)|}$. Thus, we consider two variants of the greedy function:
	\begin{equation}\label{eq:greedyfun3}
	g_2(s^{ps}; v)  =  \frac{\sum_{i \in s^{ps} \cup \{v\}\}} w_i}{w_{total}}+ incorrect_{total}
	\end{equation}
	and
	\begin{equation}\label{eq:greedyfun4}
	g_3(s^{ps}; v)  =    \frac{\sum_{i \in s^{ps} \cup \{v\}\}} w_i}{w_{total}}+ \frac{incorrect_{total}}{|D(P)|}
	\end{equation}
	From Equation (\ref{eq:greedyfun3}) one can see that the greedy function $g_2$ mainly depends on the second term, since the value of the first one is not greater than 1. On other words, this function prefers such guards  whose inclusion in the partial solution covers more points. On the other hand, if the inclusion of some two guards covers the same number of points, then the function suggests  the guard with lower cost. At the earliest stages of the greedy algorithm this approach will prefer those guards which cover more points of the not-yet-covered points from $D(P)$, while in a later stage, when less points remain uncovered, the function prefers the guards with lower cost. 
	
	In the Equation  (\ref{eq:greedyfun4}) the second term is also normalized and the trade off between two terms is more balanced.
	In this case, the algorithm does not  ultimately prefer any of criteria for choosing next vertex. Instead,  \fxnote{TODO...}
	
	
	Ties occurred in the search are broken by using  price-per-unit heuristic which is stated as follows.
	For each not yet considered guards $p_i$, we assign the region which is visible from $p_i$ by $\emph{Surf}(v_j)$. As the next candidate to extend $s^{ps}$, we choose a guard $v^*$ which covers set $V(j)$ by the smallest price per unit, among the other candidate to extend $s^{ps}$. More precisely, this greedy criterion $g$ is given as:
	\begin{align}
	g(s^{ps}, v_i) = \frac{w_{i}}{|Surf({v_i})|},
	\end{align}
	and vertex $v^*$ that minimizes $g(s^{ps}, v_i)$ is chosen as an extension of the current partial solution.
	In our experimental studies, we found out that this heuristic does not perform well on its own, but presents a reasonable tie-breaking mechanism and is able to boost quality of our greedy heuristics which includes criteria~(\ref{eq:greedyfun3}) and (\ref{eq:greedyfun4}).
	\subsubsection{Partial calculation of objective function}
	
	In order to enable fast calculation of greedy functions described in previous subsections, we introduce several useful structures and auxiliary functions:
	\begin{itemize}
		\item structure \texttt{map<Point,list<Point>> Visibility} -- for each point of $D(P)$, we provide the list of guards which are visible from that point;
		\item structure \texttt{map<Point,int> numberOfGuards} -- for each point from $D(P)$,  we keep the number of Guards in solution which are visible from that point;
		\item structure \texttt{set<Point> CoveredPoints} -- set of points covered by the partial solution;
		\item function \texttt{updateCoveredPointsAdd (vertex v)} -- updates  structures \texttt{CoveredPoints} and \texttt{numberOfGuards} by considering all new points which are covered by vertex \texttt{v}, when \texttt{v} is added to solution;
		\item function \texttt{updateCoveredPointsRemove(vertex v)} -- updates  structures \texttt{CoveredPoints} and \texttt{numberOfGuards} by removing all points which are covered by \texttt{v}, when \texttt{v} is removed from partial solution.
	\end{itemize}
	\begin{algorithm}[!t]
          	\caption{Function updateCoveredPointsAdd}\label{alg:updateCoveredPointsAdd}
          	\begin{algorithmic}[1]
          		\State \textbf{Input:} vertex $v$
          		\State \textbf{Output:} updated structures: coveredPoints and numberOfGuards
          		\For{$p$ in $S[v]$}
          		\State coveredPoints.insert($p$)
          		\State numberOfGuards.find($p$).second++
          		\EndFor
          	\end{algorithmic}
          \end{algorithm}
                    \begin{algorithm}[!t]
          	\caption{Function updateCoveredPointsAdd}\label{alg:updateCoveredPointsRemove}
          	\begin{algorithmic}[1]
          		\State \textbf{Input:} vertex $v$
          		\State \textbf{Output:} updated structures coveredPoints and numberOfGuards
          		\For{$p \in S[v]$}
          		\State numberOfGuards.find($p$).second-- --
                \If {numberOfGuards.find($p$).second$=0$}
                    \State coveredPoints.remove($p$)
                    \EndIf
          		\EndFor
          	\end{algorithmic}
          \end{algorithm}
	
	\subsubsection{A hybrid of the Greedy and a shaking}
	\fxnote{TODO: Dragan}
The idea of shaking procedure is to improve the quality of the overall search process by examining other (partial) solutions from the neighborhoods of the current (partial) solution. More precisely,
shaking procedure creates a system of neighborhoods which is used for deriving new partial solutions based on the current one. The main principle  is to remove some vertices from the current solution and to add some others, with the hope that the overall quality of the solution is improved.
 The overall procedure is shown in Figure XXX. In the $k$th neighborhood, the procedure chooses some $k$ vertices according to the specific criteria and removes them from the current solution. After that, the procedure is trying to add (not necessarily new) $k$ vertices to the solution, by using the same criterion as in the greedy function. After the process of removing and adding the vertices is finished, the algorithm examines the quality of the obtained new partial solution. Two criteria are considered: the value of the objective function (i.e. the sum of vertex weights in the solution) and the total number of uncovered points. If the both values of the new partial solution are lower  than the corresponding values of the current one, then the new solution becomes the current partial solution and the algorithm proceeds with the next neighborhood. If any of the two values of the new solution is not lower than of the corresponding values of the current one, than no changes are made and the algorithm proceeds with the next neighborhood. The algorithm stops once all neighborhoods are examined and the overall search continues with the greedy heuristic. In the proposed shaking algorithm, the minimum size of the neighborhood is 1 and the maximum size is set to $solution.size()/3$.\fxnote{sta je solution, nigdje prethodno definisano, donji cio dio nedostaje}

The choice of vertices to be removed is carefully designed and depends on geometrical properties of the considered polygon. In order to remove $k$ vertices, the algorithm iteratively call the function \texttt{removeVertex} which uses two parameters: \texttt{DISTANCE} and \texttt{OFFSET}. In each call, function \texttt{removeVertex} chooses a random vertex $v$ from the solution and forms a list of candidates for removing from the solution. The list is formed as follows. Moving along the \texttt{OFFSET} edges of the polygon from the (randomly chosen) vertex $v$ in both directions (clockwise and counterclockwise), all vertices belonging to the partial solution are initially added to the list. Further, all such vertices, which are on the Euclidean distance lower than \texttt{DISTANCE} from vertex $v$ are removed. Other words, candidate for the removing are relatively close to the vertex $v$ in the sense of adjacent edges in the polygon, but far from $v$ with respect to Euclidean distance. By this approach, shaking effect is stronger, since we remove vertices which are more distant from the current one, but still related to it with respect of the polygon structure. If such strategy leads to an empty list, then a random vertex from the solution has been removed.
\fxnote{Negdje navesti kako se racunaju ove DISTANCE and OFFSET}	
	
\subsubsection{A Hybrid of the Greedy + CPLEX}  
	The performance of \textrm{Cplex}  soon or later degrades w.r.t. instance size since the prolem is NP--hard. On the other hand, in the later stage, there is an increased chance for Greedy to worsen the final greedy solution due to a high number of components which are similar to be chosen for extension. So, it makes sense to combine  partial solutions generated with a Greedy procedure over a few interactions  and only then to use CPLEX to do the completion of the partial solution. In details, our approach consists of the following steps:
	\begin{enumerate}
		\item Run a Greedy method up to $K$ iterations (parameter) to obtain a partial solution $s^{ps}$ (therefore, $|s^{ps}| = K$);
		\item Take solution $s^{ps}$ and make it complete by solving a corresponding submodel via CPLEX:
		\begin{itemize}
			\item CPLEX solves corresponding sub-model which is formed by adding constraints $x_{i} = 1$, for all $v_i \in s^{ps}$ into the existing WOAGP model;
			\item a complete solution $\bar {s^p}$ is obtained;
		\end{itemize}
		\item Return $f(\bar {s^p})$.
	\end{enumerate}
	
	\begin{comment}
	
	\noindent \textbf{Improvements of the above method.} The above method can serve as a basic iteration
	of a more advanced techniques like ILP-LNS or CMSA. In this case, methods for destructing the solutions
	has to be proposed.  Underlying idea could be:
	\begin{itemize}
	\item remove $N$ guards with the largest costs out of $C'$
	\item remove $N$ guards which have a higher amount of points from $D(P)$ covered by other guards, represented by the function
	\begin{align}
	ratio(i) = \frac{\sum_{v \in V\setminus{ \{i\}}, j \in V(i)} 1_{j \mbox{ is veasible from } v} }{|V(i)|}.
	\end{align}
	\end{itemize}
	\end{comment}
	
	\section{Computational Results}
	We used the instance of the specific OAGP up to a 200 vertices, and assigned the weights to each vertex of polygons. We included two kind of weights into these benchmarks:
	\begin{itemize}
		\item \emph{topologically-based benchmarks}. For each vertex $i$ of polygon $P$ let us denote by $l_i$ and $l_{i+1}$ the lengths of edges that comes out of vertex $i$. Then, $w_i := \frac{l_i + l_{i+1}}{2}$. This way of assigning weights is augmented by the fact that if the the arithmetic length of both edges that comes out of vertex $i$ is longer, it is expected that vertex is a guard can see a larger pieces of polygon $P$. This implies that the range of camera $i$ has to be larger, which again means that it has a higher price. In case of the second smaller edge, we add the arithmetic mean for the both lengths as a price of the vertex.  
		\item \emph{point-based benchmarks:} For each vertex $j$, we consider the number of points in $D(P)$ that are visible from the given vertex $(|V(j)|)$. Based on this number, we assign prices to the vertices on the following way:
		\begin{equation}
		w_j = n \cdot \frac{|V(j)|}{|D(P)|}, j=1,...,n.
		\end{equation}
	\end{itemize}
     We used two different kind of benchmark sets:
     \begin{itemize}
     	 \item \emph{small} instances:\fxnote{TODO:Milan: main characteristics, why they differ from others}
     	 \item \emph{large} instances: \fxnote{TODO: Milan: main characteristics, why they differ from others}
     \end{itemize}
     For small and large instances, one polygon for each $n \in\{8,10,...,200\}$ has been generated, which makes 100 instances per each benchmark set. In overall, we have 200 polygons which vertices with two different weights, which gives us, in overall, 400 problem instances.
     \subsection{Settings and the Parameters' choice}
     \fxnote{TODO: Marko}
     
	\subsection{Results on exact methods}
\fxnote{Marko: Solving of CP i MIP solvers are limited to 5 mins. }
	\section{Conclusions and Future Work}
	
	
	
	\bibliographystyle{abbrv}
	\bibliography{bib}
	
	
\end{document}
